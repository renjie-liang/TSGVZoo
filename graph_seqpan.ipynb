{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.layers import Embedding, VisualProjection, FeatureEncoder, CQAttention, CQConcatenate, Conv1D, SeqPANPredictor\n",
    "from models.layers import DualAttentionBlock\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SeqPAN(nn.Module):\n",
    "    def __init__(self, configs, word_vectors):\n",
    "        super(SeqPAN, self).__init__()\n",
    "        self.configs = configs\n",
    "        dim = configs.model.dim\n",
    "        droprate = configs.model.droprate\n",
    "\n",
    "        max_pos_len = self.configs.max_pos_len\n",
    "        self.text_encoder = Embedding(num_words=configs.num_words, num_chars=configs.num_chars, out_dim=dim,\n",
    "                                       word_dim=configs.model.word_dim, \n",
    "                                       char_dim=configs.model.char_dim, \n",
    "                                       word_vectors=word_vectors,\n",
    "                                       droprate=droprate)\n",
    "                                       \n",
    "        self.video_affine = VisualProjection(visual_dim=configs.model.video_feature_dim, dim=dim,\n",
    "                                             droprate=droprate)\n",
    "        # self.feat_encoder = FeatureEncoder(dim=dim, kernel_size=7, num_layers=4,\n",
    "        #                                       max_pos_len=max_pos_len, droprate=droprate)\n",
    "\n",
    "\n",
    "        # self.dual_attention_block_1 = DualAttentionBlock(configs=configs, dim=dim, num_heads=configs.model.num_heads, \n",
    "        #                                                 droprate=droprate, use_bias=True, activation=None)\n",
    "        # self.dual_attention_block_2 = DualAttentionBlock(configs=configs, dim=dim, num_heads=configs.model.num_heads, \n",
    "        #                                             droprate=droprate, use_bias=True, activation=None)\n",
    "\n",
    "\n",
    "\n",
    "        # # self.tfeat_encoder = FeatureEncoder(dim=dim, num_heads=configs.model.num_heads, kernel_size=7, num_layers=4,\n",
    "        # #                                       max_pos_len=max_pos_len, droprate=droprate)\n",
    "\n",
    "        # self.q2v_attn = CQAttention(dim=dim, droprate=droprate)\n",
    "        # self.v2q_attn = CQAttention(dim=dim, droprate=droprate)\n",
    "        # self.cq_cat = CQConcatenate(dim=dim)\n",
    "        # self.match_conv1d = Conv1D(in_dim=dim, out_dim=4)\n",
    "\n",
    "        # lable_emb = torch.empty(size=[dim, 4], dtype=torch.float32)\n",
    "        # lable_emb = torch.nn.init.orthogonal_(lable_emb.data)\n",
    "        # self.label_embs = nn.Parameter(lable_emb, requires_grad=True)\n",
    "        \n",
    "        # self.predictor = SeqPANPredictor(configs)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, word_ids, char_ids, vfeat_in, vmask, tmask):\n",
    "        B = vmask.shape[0]\n",
    "        # tfeat = self.text_encoder(word_ids, char_ids)\n",
    "        vfeat = self.video_affine(vfeat_in)\n",
    "        return vfeat\n",
    "        # vfeat = self.feat_encoder(vfeat)\n",
    "        # tfeat = self.feat_encoder(tfeat)\n",
    "\n",
    "        # vfeat_ = self.dual_attention_block_1(vfeat, tfeat, vmask, tmask)\n",
    "        # tfeat_ = self.dual_attention_block_1(tfeat, vfeat, tmask, vmask)\n",
    "        # vfeat = vfeat_\n",
    "        # tfeat = tfeat_\n",
    "\n",
    "        # vfeat_ = self.dual_attention_block_2(vfeat, tfeat, vmask, tmask)\n",
    "        # tfeat_ = self.dual_attention_block_2(tfeat, vfeat, tmask, vmask)\n",
    "\n",
    "        # vfeat = vfeat_\n",
    "        # tfeat = tfeat_\n",
    "\n",
    "\n",
    "        # t2v_feat = self.q2v_attn(vfeat, tfeat, vmask, tmask)\n",
    "        # v2t_feat = self.v2q_attn(tfeat, vfeat, tmask, vmask)\n",
    "        \n",
    "        # fuse_feat = self.cq_cat(t2v_feat, v2t_feat, tmask)\n",
    "\n",
    "        # match_logits = self.match_conv1d(fuse_feat)\n",
    "\n",
    "\n",
    "        # match_score = F.gumbel_softmax(match_logits, tau=0.3)\n",
    "        # match_probs =torch.log(match_score)\n",
    "\n",
    "\n",
    "\n",
    "        # soft_label_embs = torch.matmul(match_score, torch.tile(self.label_embs, (B, 1, 1)).permute(0, 2, 1))\n",
    "\n",
    "        # fuse_feat = (fuse_feat + soft_label_embs) * vmask.unsqueeze(2)\n",
    "        # start_logits, end_logits = self.predictor(fuse_feat, vmask)\n",
    "        # return start_logits, end_logits, match_probs, self.label_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchsummary. See above stack traces for more details. Executed layers up to: [WordEmbedding: 2-1, Dropout: 3-1, Embedding: 3-2, Dropout: 3-3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py3/lib/python3.9/site-packages/torchsummary/torchsummary.py:140\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 140\u001b[0m         _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)(\u001b[39m*\u001b[39;49mx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/3_ActiveLearn/seqpan_pytorch/models/model.py:54\u001b[0m, in \u001b[0;36mSeqPAN.forward\u001b[0;34m(self, word_ids, char_ids, vfeat_in, vmask, tmask)\u001b[0m\n\u001b[1;32m     52\u001b[0m B \u001b[39m=\u001b[39m vmask\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 54\u001b[0m tfeat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder(word_ids, char_ids)\n\u001b[1;32m     55\u001b[0m vfeat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvideo_affine(vfeat_in)\n",
      "File \u001b[0;32m~/anaconda3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/3_ActiveLearn/seqpan_pytorch/models/layers.py:88\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, word_ids, char_ids)\u001b[0m\n\u001b[1;32m     87\u001b[0m word_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_emb(word_ids)  \u001b[39m# (batch_size, w_seq_len, word_dim)\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m char_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchar_emb(char_ids)  \u001b[39m# (batch_size, w_seq_len, 100)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([word_emb, char_emb], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# (batch_size, w_seq_len, word_dim + 100)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/3_ActiveLearn/seqpan_pytorch/models/layers.py:67\u001b[0m, in \u001b[0;36mCharacterEmbedding.forward\u001b[0;34m(self, char_ids)\u001b[0m\n\u001b[1;32m     66\u001b[0m char_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(char_emb)\n\u001b[0;32m---> 67\u001b[0m char_emb \u001b[39m=\u001b[39m char_emb\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)  \u001b[39m# (batch_size, char_dim, w_seq_len, c_seq_len)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m char_outputs \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/storage/rjliang/3_ActiveLearn/seqpan_pytorch/graph_seqpan.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Basus01/storage/rjliang/3_ActiveLearn/seqpan_pytorch/graph_seqpan.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m configs\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Basus01/storage/rjliang/3_ActiveLearn/seqpan_pytorch/graph_seqpan.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m=\u001b[39mSeqPAN(configs, dataset[\u001b[39m'\u001b[39m\u001b[39mword_vector\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Basus01/storage/rjliang/3_ActiveLearn/seqpan_pytorch/graph_seqpan.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m summary(model, [[\u001b[39m1\u001b[39;49m, \u001b[39m11\u001b[39;49m], [\u001b[39m1\u001b[39;49m, \u001b[39m11\u001b[39;49m, \u001b[39m12\u001b[39;49m], [\u001b[39m1\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m1024\u001b[39;49m], [\u001b[39m1\u001b[39;49m, \u001b[39m64\u001b[39;49m], [\u001b[39m1\u001b[39;49m, \u001b[39m11\u001b[39;49m]], dtypes\u001b[39m=\u001b[39;49m[torch\u001b[39m.\u001b[39;49mlong, torch\u001b[39m.\u001b[39;49mlong, torch\u001b[39m.\u001b[39;49mfloat, torch\u001b[39m.\u001b[39;49mfloat, torch\u001b[39m.\u001b[39;49mfloat])\n",
      "File \u001b[0;32m~/anaconda3/envs/py3/lib/python3.9/site-packages/torchsummary/torchsummary.py:143\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    142\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchsummary. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(executed_layers)\n\u001b[1;32m    146\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m hooks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchsummary. See above stack traces for more details. Executed layers up to: [WordEmbedding: 2-1, Dropout: 3-1, Embedding: 3-2, Dropout: 3-3]"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import tensorwatch as tw\n",
    "from easydict import EasyDict\n",
    "from utils.data_gen import load_dataset\n",
    "from utils.utils import load_json\n",
    "configs = EasyDict(load_json(\"config/charades/main_c3dFT.json\"))\n",
    "configs.suffix = \"\"\n",
    "dataset = load_dataset(configs)\n",
    "configs.num_chars = dataset['n_chars']\n",
    "configs.num_words = dataset['n_words']\n",
    "configs.train.batch_size = 1\n",
    "\n",
    "model=SeqPAN(configs, dataset['word_vector'])\n",
    "summary(model, [[1, 11], [1, 11, 12], [1, 64, 1024], [1, 64], [1, 11]], dtypes=[torch.long, torch.long, torch.float, torch.float, torch.float])\n",
    "# '''tensorboardX生成日志文件'''\n",
    "# dummy_input01 = torch.rand(10, 1, 28, 28)  # 假设输入10张1*28*28的图片\n",
    "# dummy_input02 = torch.rand(10, 1, 28, 28)  # 假设输入10张1*28*28的图片\n",
    "# with SummaryWriter(comment='Net') as w:\n",
    "#     w.add_graph(Net, (dummy_input01,dummy_input02))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = torch.rand([1, 11])\n",
    "char_ids = torch.rand([1, 11, 12]).long()\n",
    "vfeats = torch.rand( [1, 64, 1024]).float()\n",
    "vmask = torch.rand( [1, 64]).float()\n",
    "tmask = torch.rand( [1, 11]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(word_ids, char_ids, vfeats, vmask, tmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import SeqPAN\n",
    "model=SeqPAN(configs, dataset['word_vector'])\n",
    "# print(model)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_embs : torch.Size([128, 4])\n",
      "text_encoder.word_emb.pad_vec : torch.Size([1, 300])\n",
      "text_encoder.word_emb.unk_vec : torch.Size([1, 300])\n",
      "text_encoder.word_emb.glove_vec : torch.Size([1269, 300])\n",
      "text_encoder.char_emb.char_emb.weight : torch.Size([36, 100])\n",
      "text_encoder.char_emb.char_convs.0.0.weight : torch.Size([10, 100, 1, 1])\n",
      "text_encoder.char_emb.char_convs.0.0.bias : torch.Size([10])\n",
      "text_encoder.char_emb.char_convs.1.0.weight : torch.Size([20, 100, 1, 2])\n",
      "text_encoder.char_emb.char_convs.1.0.bias : torch.Size([20])\n",
      "text_encoder.char_emb.char_convs.2.0.weight : torch.Size([30, 100, 1, 3])\n",
      "text_encoder.char_emb.char_convs.2.0.bias : torch.Size([30])\n",
      "text_encoder.char_emb.char_convs.3.0.weight : torch.Size([40, 100, 1, 4])\n",
      "text_encoder.char_emb.char_convs.3.0.bias : torch.Size([40])\n",
      "text_encoder.query_conv1d.conv1d.weight : torch.Size([128, 400, 1])\n",
      "text_encoder.query_conv1d.conv1d.bias : torch.Size([128])\n",
      "text_encoder.q_layer_norm.weight : torch.Size([128])\n",
      "text_encoder.q_layer_norm.bias : torch.Size([128])\n",
      "video_affine.video_conv1d.conv1d.weight : torch.Size([128, 1024, 1])\n",
      "video_affine.video_conv1d.conv1d.bias : torch.Size([128])\n",
      "video_affine.v_layer_norm.weight : torch.Size([128])\n",
      "video_affine.v_layer_norm.bias : torch.Size([128])\n",
      "feat_encoder.pos_embedding.position_embeddings.weight : torch.Size([64, 128])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.0.0.weight : torch.Size([128, 1, 7])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.0.1.weight : torch.Size([128, 128, 1])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.0.1.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.1.0.weight : torch.Size([128, 1, 7])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.1.1.weight : torch.Size([128, 128, 1])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.1.1.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.2.0.weight : torch.Size([128, 1, 7])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.2.1.weight : torch.Size([128, 128, 1])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.2.1.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.3.0.weight : torch.Size([128, 1, 7])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.3.1.weight : torch.Size([128, 128, 1])\n",
      "feat_encoder.conv_block.depthwise_separable_conv.3.1.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.0.weight : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.0.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.1.weight : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.1.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.2.weight : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.2.bias : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.3.weight : torch.Size([128])\n",
      "feat_encoder.conv_block.layer_norms.3.bias : torch.Size([128])\n",
      "dual_attention_block_1.layer_norm_1.weight : torch.Size([128])\n",
      "dual_attention_block_1.layer_norm_1.bias : torch.Size([128])\n",
      "dual_attention_block_1.layer_norm_2.weight : torch.Size([128])\n",
      "dual_attention_block_1.layer_norm_2.bias : torch.Size([128])\n",
      "dual_attention_block_1.layer_norm_t.weight : torch.Size([128])\n",
      "dual_attention_block_1.layer_norm_t.bias : torch.Size([128])\n",
      "dual_attention_block_1.dense_1.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dense_1.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dense_2.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dense_2.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.query.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.query.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.f_key.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.f_key.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.f_value.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.f_value.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.t_key.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.t_key.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.t_value.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.t_value.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.s_dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.s_dense.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.x_dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.x_dense.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.s_gate.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.s_gate.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.x_gate.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.x_gate.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.guided_dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.guided_dense.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_1.bias_value : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_1.dense_1.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_1.dense_1.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_1.dense_2.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_1.dense_2.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_2.bias_value : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_2.dense_1.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_2.dense_1.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_2.dense_2.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.bilinear_2.dense_2.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.layer_norm1.weight : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.layer_norm1.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.layer_norm2.weight : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.layer_norm2.bias : torch.Size([128])\n",
      "dual_attention_block_1.dual_multihead_attention.out_layer.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_1.dual_multihead_attention.out_layer.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.layer_norm_1.weight : torch.Size([128])\n",
      "dual_attention_block_2.layer_norm_1.bias : torch.Size([128])\n",
      "dual_attention_block_2.layer_norm_2.weight : torch.Size([128])\n",
      "dual_attention_block_2.layer_norm_2.bias : torch.Size([128])\n",
      "dual_attention_block_2.layer_norm_t.weight : torch.Size([128])\n",
      "dual_attention_block_2.layer_norm_t.bias : torch.Size([128])\n",
      "dual_attention_block_2.dense_1.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dense_1.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dense_2.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dense_2.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.query.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.query.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.f_key.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.f_key.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.f_value.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.f_value.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.t_key.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.t_key.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.t_value.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.t_value.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.s_dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.s_dense.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.x_dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.x_dense.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.s_gate.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.s_gate.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.x_gate.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.x_gate.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.guided_dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.guided_dense.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_1.bias_value : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_1.dense_1.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_1.dense_1.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_1.dense_2.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_1.dense_2.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_2.bias_value : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_2.dense_1.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_2.dense_1.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_2.dense_2.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.bilinear_2.dense_2.conv1d.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.layer_norm1.weight : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.layer_norm1.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.layer_norm2.weight : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.layer_norm2.bias : torch.Size([128])\n",
      "dual_attention_block_2.dual_multihead_attention.out_layer.conv1d.weight : torch.Size([128, 128, 1])\n",
      "dual_attention_block_2.dual_multihead_attention.out_layer.conv1d.bias : torch.Size([128])\n",
      "q2v_attn.w4C : torch.Size([128, 1])\n",
      "q2v_attn.w4Q : torch.Size([128, 1])\n",
      "q2v_attn.w4mlu : torch.Size([1, 1, 128])\n",
      "q2v_attn.cqa_linear.conv1d.weight : torch.Size([128, 512, 1])\n",
      "q2v_attn.cqa_linear.conv1d.bias : torch.Size([128])\n",
      "v2q_attn.w4C : torch.Size([128, 1])\n",
      "v2q_attn.w4Q : torch.Size([128, 1])\n",
      "v2q_attn.w4mlu : torch.Size([1, 1, 128])\n",
      "v2q_attn.cqa_linear.conv1d.weight : torch.Size([128, 512, 1])\n",
      "v2q_attn.cqa_linear.conv1d.bias : torch.Size([128])\n",
      "cq_cat.weighted_pool.weight : torch.Size([128, 1])\n",
      "cq_cat.conv1d.conv1d.weight : torch.Size([128, 256, 1])\n",
      "cq_cat.conv1d.conv1d.bias : torch.Size([128])\n",
      "match_conv1d.conv1d.weight : torch.Size([4, 128, 1])\n",
      "match_conv1d.conv1d.bias : torch.Size([4])\n",
      "predictor.feature_encoder.pos_embedding.position_embeddings.weight : torch.Size([64, 128])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.0.0.weight : torch.Size([128, 1, 7])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.0.1.weight : torch.Size([128, 128, 1])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.0.1.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.1.0.weight : torch.Size([128, 1, 7])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.1.1.weight : torch.Size([128, 128, 1])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.1.1.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.2.0.weight : torch.Size([128, 1, 7])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.2.1.weight : torch.Size([128, 128, 1])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.2.1.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.3.0.weight : torch.Size([128, 1, 7])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.3.1.weight : torch.Size([128, 128, 1])\n",
      "predictor.feature_encoder.conv_block.depthwise_separable_conv.3.1.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.0.weight : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.0.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.1.weight : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.1.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.2.weight : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.2.bias : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.3.weight : torch.Size([128])\n",
      "predictor.feature_encoder.conv_block.layer_norms.3.bias : torch.Size([128])\n",
      "predictor.feature_encoder.layer_norm_1.weight : torch.Size([128])\n",
      "predictor.feature_encoder.layer_norm_1.bias : torch.Size([128])\n",
      "predictor.feature_encoder.layer_norm_2.weight : torch.Size([128])\n",
      "predictor.feature_encoder.layer_norm_2.bias : torch.Size([128])\n",
      "predictor.feature_encoder.top_self_attention.selfattn.in_proj_weight : torch.Size([384, 128])\n",
      "predictor.feature_encoder.top_self_attention.selfattn.in_proj_bias : torch.Size([384])\n",
      "predictor.feature_encoder.top_self_attention.selfattn.out_proj.weight : torch.Size([128, 128])\n",
      "predictor.feature_encoder.top_self_attention.selfattn.out_proj.bias : torch.Size([128])\n",
      "predictor.feature_encoder.dense.conv1d.weight : torch.Size([128, 128, 1])\n",
      "predictor.feature_encoder.dense.conv1d.bias : torch.Size([128])\n",
      "predictor.start_layer_norm.weight : torch.Size([128])\n",
      "predictor.start_layer_norm.bias : torch.Size([128])\n",
      "predictor.end_layer_norm.weight : torch.Size([128])\n",
      "predictor.end_layer_norm.bias : torch.Size([128])\n",
      "predictor.start_hidden.conv1d.weight : torch.Size([128, 256, 1])\n",
      "predictor.start_hidden.conv1d.bias : torch.Size([128])\n",
      "predictor.end_hidden.conv1d.weight : torch.Size([128, 256, 1])\n",
      "predictor.end_hidden.conv1d.bias : torch.Size([128])\n",
      "predictor.start_dense.conv1d.weight : torch.Size([1, 128, 1])\n",
      "predictor.start_dense.conv1d.bias : torch.Size([1])\n",
      "predictor.end_dense.conv1d.weight : torch.Size([1, 128, 1])\n",
      "predictor.end_dense.conv1d.bias : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85417aaa996bf384aabef58b87c77e6a0c178545399cda58bd1fcf7e684b91db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
